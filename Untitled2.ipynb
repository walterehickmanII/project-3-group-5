{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbH8FQp9uB3n",
        "outputId": "cd367108-ae9f-4c3b-da90-d23e39fcc975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install gdown\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/drive/MyDrive/Project3database.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure 'Year' and 'Tm' columns are present\n",
        "if 'Year' not in data.columns or 'Tm' not in data.columns:\n",
        "    raise ValueError(\"'Year' and 'Tm' columns are required in the dataset.\")\n",
        "\n",
        "# Select relevant features for prediction\n",
        "features = ['R/G', 'G', 'PA', 'AB', 'RS', 'H', '2B', '3B', 'HR', 'RBI', 'SB', 'CS', 'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS']\n",
        "\n",
        "# Ensure there are no missing values in the features\n",
        "data_filtered = data.dropna(subset=features)\n",
        "\n",
        "# Generate random binary target data for demonstration purposes (since the real target data is cleared)\n",
        "data_filtered['Playoff'] = np.random.randint(2, size=len(data_filtered))\n",
        "data_filtered['Champion'] = np.random.randint(2, size=len(data_filtered))\n",
        "\n",
        "# Define the feature set and target variables\n",
        "X = data_filtered[features]\n",
        "y_playoff = data_filtered['Playoff']\n",
        "y_champion = data_filtered['Champion']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets for playoff prediction\n",
        "X_train, X_test, y_playoff_train, y_playoff_test = train_test_split(X_scaled, y_playoff, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForestClassifier model for playoff prediction\n",
        "model_playoff = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_playoff.fit(X_train, y_playoff_train)\n",
        "\n",
        "# Make predictions on the test set for playoff prediction\n",
        "y_playoff_pred = model_playoff.predict(X_test)\n",
        "playoff_accuracy = accuracy_score(y_playoff_test, y_playoff_pred)\n",
        "print(\"Playoff Prediction Accuracy:\", playoff_accuracy)\n",
        "\n",
        "# Evaluate the playoff model\n",
        "print(\"Random Forest Classifier Evaluation for Playoff Prediction\")\n",
        "print(classification_report(y_playoff_test, y_playoff_pred))\n",
        "print(confusion_matrix(y_playoff_test, y_playoff_pred))\n",
        "\n",
        "# Split the data into training and testing sets for champion prediction\n",
        "_, _, y_champion_train, y_champion_test = train_test_split(X_scaled, y_champion, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForestClassifier model for champion prediction\n",
        "model_champion = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_champion.fit(X_train, y_champion_train)\n",
        "\n",
        "# Make predictions on the test set for champion prediction\n",
        "y_champion_pred = model_champion.predict(X_test)\n",
        "champion_accuracy = accuracy_score(y_champion_test, y_champion_pred)\n",
        "print(\"Champion Prediction Accuracy:\", champion_accuracy)\n",
        "\n",
        "# Evaluate the champion model\n",
        "print(\"Random Forest Classifier Evaluation for Champion Prediction\")\n",
        "print(classification_report(y_champion_test, y_champion_pred))\n",
        "print(confusion_matrix(y_champion_test, y_champion_pred))\n",
        "\n",
        "# Neural network model for playoff prediction\n",
        "model_playoff_nn = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_playoff_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_playoff_nn.fit(X_train, y_playoff_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the neural network model for playoff prediction\n",
        "playoff_nn_accuracy = model_playoff_nn.evaluate(X_test, y_playoff_test)\n",
        "print(\"Playoff Neural Network Prediction Accuracy:\", playoff_nn_accuracy[1])\n",
        "\n",
        "# Neural network model for champion prediction\n",
        "model_champion_nn = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_champion_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_champion_nn.fit(X_train, y_champion_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the neural network model for champion prediction\n",
        "champion_nn_accuracy = model_champion_nn.evaluate(X_test, y_champion_test)\n",
        "print(\"Champion Neural Network Prediction Accuracy:\", champion_nn_accuracy[1])\n",
        "\n",
        "# Function to predict playoff teams and champion for a given year\n",
        "def predict_for_year(year):\n",
        "    if year not in data['Year'].values:\n",
        "        print(f\"The year {year} is not in the dataset.\")\n",
        "        return\n",
        "\n",
        "    filtered_data = data[data['Year'] == year].copy()\n",
        "\n",
        "    # Ensure the same columns, in the same order, are used when transforming `future_data`\n",
        "    relevant_columns = X.columns\n",
        "    future_data = filtered_data[relevant_columns]\n",
        "\n",
        "    # Standardize the features\n",
        "    future_data_scaled = scaler.transform(future_data)\n",
        "\n",
        "    # Get the required number of playoff teams based on the year\n",
        "    if 2000 <= year <= 2011:\n",
        "        n_playoff_teams = 8\n",
        "    elif 2012 <= year <= 2019:\n",
        "        n_playoff_teams = 10\n",
        "    elif year == 2020:\n",
        "        n_playoff_teams = 16\n",
        "    elif 2022 <= year <= 2024:\n",
        "        n_playoff_teams = 12\n",
        "    else:\n",
        "        raise ValueError(\"Year is out of the expected range for this analysis\")\n",
        "\n",
        "    # Predict playoff teams\n",
        "    future_playoff_predictions_proba = model_playoff.predict_proba(future_data_scaled)[:, 1]\n",
        "    filtered_data['Playoff_Prob'] = future_playoff_predictions_proba\n",
        "\n",
        "    # Select top N teams based on playoff probability\n",
        "    playoff_teams = filtered_data.nlargest(n_playoff_teams, 'Playoff_Prob')\n",
        "    print(\"Teams predicted to be in playoffs:\")\n",
        "    print(playoff_teams[['Tm', 'Playoff_Prob']])\n",
        "\n",
        "    # Predict champions among the playoff teams\n",
        "    future_playoff_data = playoff_teams[relevant_columns]\n",
        "    future_playoff_data_scaled = scaler.transform(future_playoff_data)\n",
        "\n",
        "    future_champion_predictions_proba = model_champion.predict_proba(future_playoff_data_scaled)[:, 1]\n",
        "    playoff_teams['Champion_Prob'] = future_champion_predictions_proba\n",
        "\n",
        "    # Select the champion (team with the highest probability) among playoff teams\n",
        "    champion_team = playoff_teams.loc[playoff_teams['Champion_Prob'].idxmax()]\n",
        "    print(\"Team predicted to be the champion:\")\n",
        "    print(champion_team[['Tm', 'Champion_Prob']])\n",
        "\n",
        "# Ask the user for a year and make predictions\n",
        "try:\n",
        "    year_to_predict = int(input(\"Enter a year to predict playoffs and champion: \"))\n",
        "    predict_for_year(year_to_predict)\n",
        "except ValueError:\n",
        "    print(\"Please enter a valid year.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os\n",
        "import openai\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Download VADER lexicon (only need to do this once)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Google Drive file ID (part of the shareable link)\n",
        "file_id = '1p_ab8FVdg2ljxhrudkPNTETyoRLj6Ab9'\n",
        "output = 'env.text'\n",
        "\n",
        "# Construct the download URL\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Download the file from Google Drive\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Read the API key from the file\n",
        "with open('env.text', 'r') as file:\n",
        "    api_key = file.read().strip()\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "print(\"API Key successfully set!\")\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/drive/MyDrive/Project3database.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Ensure 'Year' and 'Tm' columns are present\n",
        "if 'Year' not in data.columns or 'Tm' not in data.columns:\n",
        "    raise ValueError(\"'Year' and 'Tm' columns are required in the dataset.\")\n",
        "\n",
        "# Select relevant features for prediction\n",
        "features = ['R/G', 'G', 'PA', 'AB', 'RS', 'H', '2B', '3B', 'HR', 'RBI', 'SB', 'CS', 'BB', 'SO', 'BA', 'OBP', 'SLG', 'OPS']\n",
        "\n",
        "# Ensure there are no missing values in the features\n",
        "data_filtered = data.dropna(subset=features)\n",
        "\n",
        "# Generate random binary target data for demonstration purposes (since the real target data is cleared)\n",
        "data_filtered['Playoff'] = np.random.randint(2, size=len(data_filtered))\n",
        "data_filtered['Champion'] = np.random.randint(2, size=len(data_filtered))\n",
        "\n",
        "# Define the feature set and target variables\n",
        "X = data_filtered[features]\n",
        "y_playoff = data_filtered['Playoff']\n",
        "y_champion = data_filtered['Champion']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and testing sets for playoff prediction\n",
        "X_train, X_test, y_playoff_train, y_playoff_test = train_test_split(X_scaled, y_playoff, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForestClassifier model for playoff prediction\n",
        "model_playoff = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_playoff.fit(X_train, y_playoff_train)\n",
        "\n",
        "# Make predictions on the test set for playoff prediction\n",
        "y_playoff_pred = model_playoff.predict(X_test)\n",
        "playoff_accuracy = accuracy_score(y_playoff_test, y_playoff_pred)\n",
        "print(\"Playoff Prediction Accuracy:\", playoff_accuracy)\n",
        "\n",
        "# Evaluate the playoff model\n",
        "print(\"Random Forest Classifier Evaluation for Playoff Prediction\")\n",
        "print(classification_report(y_playoff_test, y_playoff_pred))\n",
        "print(confusion_matrix(y_playoff_test, y_playoff_pred))\n",
        "\n",
        "# Split the data into training and testing sets for champion prediction\n",
        "_, _, y_champion_train, y_champion_test = train_test_split(X_scaled, y_champion, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a RandomForestClassifier model for champion prediction\n",
        "model_champion = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_champion.fit(X_train, y_champion_train)\n",
        "\n",
        "# Make predictions on the test set for champion prediction\n",
        "y_champion_pred = model_champion.predict(X_test)\n",
        "champion_accuracy = accuracy_score(y_champion_test, y_champion_pred)\n",
        "print(\"Champion Prediction Accuracy:\", champion_accuracy)\n",
        "\n",
        "# Evaluate the champion model\n",
        "print(\"Random Forest Classifier Evaluation for Champion Prediction\")\n",
        "print(classification_report(y_champion_test, y_champion_pred))\n",
        "print(confusion_matrix(y_champion_test, y_champion_pred))\n",
        "\n",
        "# Neural network model for playoff prediction\n",
        "model_playoff_nn = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_playoff_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_playoff_nn.fit(X_train, y_playoff_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the neural network model for playoff prediction\n",
        "playoff_nn_accuracy = model_playoff_nn.evaluate(X_test, y_playoff_test)\n",
        "print(\"Playoff Neural Network Prediction Accuracy:\", playoff_nn_accuracy[1])\n",
        "\n",
        "# Neural network model for champion prediction\n",
        "model_champion_nn = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_champion_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_champion_nn.fit(X_train, y_champion_train, epochs=50, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the neural network model for champion prediction\n",
        "champion_nn_accuracy = model_champion_nn.evaluate(X_test, y_champion_test)\n",
        "print(\"Champion Neural Network Prediction Accuracy:\", champion_nn_accuracy[1])\n",
        "\n",
        "# Sample article text for sentiment context\n",
        "article_text = \"\"\"\n",
        "The New York Yankees have had a remarkable season with outstanding performances from their star players.\n",
        "They are expected to be strong contenders in the playoffs. The Los Angeles Dodgers, on the other hand,\n",
        "have faced some challenges but are still considered one of the top teams. The Houston Astros have shown\n",
        "resilience and are expected to give tough competition. The Boston Red Sox have had a mixed season, with\n",
        "some impressive wins and disappointing losses.\n",
        "\"\"\"\n",
        "\n",
        "# Function to extract relevant text for a team\n",
        "def extract_relevant_text(team_name, article):\n",
        "    sentences = article.split('.')\n",
        "    relevant_sentences = [sentence for sentence in sentences if team_name in sentence]\n",
        "    return ' '.join(relevant_sentences)\n",
        "\n",
        "# Function to analyze sentiment\n",
        "def analyze_sentiment(text, context):\n",
        "    combined_text = f\"{context}\\n{text}\"\n",
        "    sentiment_scores = sia.polarity_scores(combined_text)\n",
        "    return sentiment_scores\n",
        "\n",
        "# Function to predict playoff teams and champion for a given year\n",
        "def predict_for_year(year):\n",
        "    if year not in data['Year'].values:\n",
        "        print(f\"The year {year} is not in the dataset.\")\n",
        "        return\n",
        "\n",
        "    filtered_data = data[data['Year'] == year].copy()  # Use .copy() to avoid the SettingWithCopyWarning\n",
        "\n",
        "    # Ensure the same columns, in the same order, are used when transforming `future_data`\n",
        "    relevant_columns = X.columns\n",
        "    future_data = filtered_data[relevant_columns]\n",
        "\n",
        "    # Standardize the features\n",
        "    future_data_scaled = scaler.transform(future_data)\n",
        "\n",
        "    # Get the required number of playoff teams based on the year\n",
        "    if 2000 <= year <= 2011:\n",
        "        n_playoff_teams = 8\n",
        "    elif 2012 <= year <= 2019:\n",
        "        n_playoff_teams = 10\n",
        "    elif year == 2020:\n",
        "        n_playoff_teams = 16\n",
        "    elif 2022 <= year <= 2024:\n",
        "        n_playoff_teams = 12\n",
        "    else:\n",
        "        raise ValueError(\"Year is out of the expected range for this analysis\")\n",
        "\n",
        "    # Predict playoff teams\n",
        "    future_playoff_predictions_proba = model_playoff.predict_proba(future_data_scaled)[:, 1]\n",
        "    filtered_data['Playoff_Prob'] = future_playoff_predictions_proba\n",
        "\n",
        "    # Select top N teams based on playoff probability\n",
        "    playoff_teams = filtered_data.nlargest(n_playoff_teams, 'Playoff_Prob')\n",
        "    print(\"Teams predicted to be in playoffs:\")\n",
        "    print(playoff_teams[['Tm', 'Playoff_Prob']])\n",
        "\n",
        "    # Predict champions among playoff teams\n",
        "    playoff_teams_scaled = scaler.transform(playoff_teams[relevant_columns])\n",
        "    future_champion_predictions_proba = model_champion.predict_proba(playoff_teams_scaled)[:, 1]\n",
        "    playoff_teams['Champion_Prob'] = future_champion_predictions_proba\n",
        "\n",
        "    # Select the champion (team with the highest probability)\n",
        "    champion_team = playoff_teams.loc[playoff_teams['Champion_Prob'].idxmax()]\n",
        "    print(\"Team predicted to be the champion:\")\n",
        "    print(champion_team[['Tm', 'Champion_Prob']])\n",
        "\n",
        "    # Extract relevant text for sentiment analysis\n",
        "    team_name = champion_team['Tm']\n",
        "    relevant_text = extract_relevant_text(team_name, article_text)\n",
        "\n",
        "    # Analyze sentiment based on the team name and article context\n",
        "    sentiment = analyze_sentiment(team_name, relevant_text)\n",
        "    print(f\"Sentiment for {team_name}: {sentiment}\")\n",
        "\n",
        "# Ask the user for a year and make predictions\n",
        "try:\n",
        "    year_to_predict = int(input(\"Enter a year to predict playoffs and champion: \"))\n",
        "    predict_for_year(year_to_predict)\n",
        "except ValueError:\n",
        "    print(\"Please enter a valid year.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
